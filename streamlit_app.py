# -*- coding: utf-8 -*-
"""
AI Script Generator (Bangla) ‚Äî External-only (no in-code samples)
- Style exemplars: Google Doc (ID or link) -> DOCX export -> extracted paragraphs
- Intent/Product samples: Google Sheet (ID or link) -> CSV export -> (intent, product, script)
- Elaborated, varied generation with anti-copy & no-facts-echo
"""

import os, re, io, time, random, difflib, requests
import pandas as pd
import streamlit as st
from urllib.parse import urlparse, parse_qs
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from docx import Document
import gdown

# =========================
# üîß OPTIONAL CONFIG (can leave blank and use the UI)
# =========================
MASTER_DOC_ID   = ""   # e.g. "1AbCdEfG..." (Google Doc ID) ‚Äî leave "" to supply in UI
MASTER_DOC_LINK = ""   # e.g. full GDoc link OR a direct .docx URL ‚Äî leave "" to supply in UI
GSHEET_ID       = ""   # e.g. "1Xyz..." (Google Sheet ID) ‚Äî leave "" to supply in UI
GSHEET_LINK     = ""   # e.g. full Google Sheet link ‚Äî leave "" to supply in UI
GSHEET_GID      = "0"  # Sheet tab gid (if using GSHEET_ID)
MODEL_NAME      = "google/flan-t5-small"  # lightweight for Streamlit Cloud / Py3.13

# =========================
# Product facts (verbatim at the end)
# =========================
PRODUCT_FACTS = {
    "UCB Income Plus Fund": {
        "indicative_return": "‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶á‡¶ô‡ßç‡¶ó‡¶ø‡¶§‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶®‡ßá‡¶ü ~‡ßØ‚Äì‡ßß‡ßß% (‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø ‡¶®‡¶Ø‡¶º)",
        "exit_load": "‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ßß‡ß¶‡ß¶ ‡¶¶‡¶ø‡¶® ‡¶∞‡¶ø‡¶°‡ßá‡¶Æ‡ßç‡¶™‡¶∂‡¶®‡ßá ‡¶ö‡¶æ‡¶∞‡ßç‡¶ú; ‡¶è‡¶∞‡¶™‡¶∞ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡¶ö‡¶æ‡¶∞‡ßç‡¶ú‡¶Æ‡ßÅ‡¶ï‡ßç‡¶§",
        "sip": "SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡ß≥‡ß´,‡ß¶‡ß¶‡ß¶/‡¶Æ‡¶æ‡¶∏",
        "non_sip": "Non-SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡ß≥‡ßß‡ß¶,‡ß¶‡ß¶‡ß¶",
        "tax": "‡¶ï‡¶∞ ‡¶∞‡¶ø‡¶¨‡ßá‡¶ü‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ (‡¶Ü‡¶Ø‡¶º‡¶ï‡¶∞ ‡¶¨‡¶ø‡¶ß‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ)"
    },
    "UCB AML First Mutual Fund": {
        "indicative_return": "‡¶á‡¶ï‡ßÅ‡¶á‡¶ü‡¶ø-‡¶Ö‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶°‚Äî‡¶¶‡ßÄ‡¶∞‡ßç‡¶ò‡¶Æ‡ßá‡¶Ø‡¶º‡¶æ‡¶¶‡ßá ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞",
        "exit_load": "‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ü ‡¶≤‡ßã‡¶° ‡¶∏‡ßç‡¶ï‡¶ø‡¶Æ ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡¶§‡ßç‡¶∞ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ",
        "sip": "SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡ß≥‡ß©,‡ß¶‡ß¶‡ß¶/‡¶Æ‡¶æ‡¶∏",
        "non_sip": "Non-SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡ß≥‡ßß‡ß¶,‡ß¶‡ß¶‡ß¶",
        "tax": "‡¶™‡ßç‡¶∞‡¶Ø‡ßã‡¶ú‡ßç‡¶Ø ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Ø‡¶º‡¶ï‡¶∞ ‡¶∞‡¶ø‡¶¨‡ßá‡¶ü"
    },
    "UCB Taqwa Growth Fund": {
        "indicative_return": "‡¶∂‡¶∞‡ßÄ‡¶Ø‡¶º‡¶æ‡¶π‡¶∏‡¶Æ‡ßç‡¶Æ‡¶§ ‡¶á‡¶ï‡ßÅ‡¶á‡¶ü‡¶ø‚Äî‡¶¶‡ßÄ‡¶∞‡ßç‡¶ò‡¶Æ‡ßá‡¶Ø‡¶º‡¶æ‡¶¶‡ßá ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞",
        "exit_load": "‡¶∏‡ßç‡¶ï‡¶ø‡¶Æ ‡¶§‡¶•‡ßç‡¶Ø‡¶™‡¶§‡ßç‡¶∞ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ü ‡¶≤‡ßã‡¶°",
        "sip": "SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡ß≥‡ß©,‡ß¶‡ß¶‡ß¶/‡¶Æ‡¶æ‡¶∏",
        "non_sip": "Non-SIP: ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ 500 unit",
        "tax": "‡¶∂‡¶∞‡ßÄ‡¶Ø‡¶º‡¶æ‡¶π ‡¶Ö‡¶®‡ßÅ‡¶ó‡¶§; ‡¶™‡ßç‡¶∞‡¶Ø‡ßã‡¶ú‡ßç‡¶Ø ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶ï‡¶∞-‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ"
    }
}

# =========================
# Utils
# =========================
def _paragraphize(txt: str) -> str:
    if not isinstance(txt, str): return ""
    txt = re.sub(r'(?m)^\s*[‚Ä¢\-\u2022]+\s*', '', txt)  # bullets
    txt = re.sub(r'(?m)^\s*\d+\.\s*', '', txt)         # 1. 2. 3.
    txt = txt.replace('‚Äî ‚Äî ‚Äî', ' ')
    txt = re.sub(r'\n{3,}', '\n\n', txt)
    return txt.strip()

def _len_ok(s: str, n: int = 420) -> bool:
    return bool(s and len(s) >= n)

def _similar(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def _sheet_id_and_gid(url_or_id: str):
    s = (url_or_id or "").strip()
    if "/" not in s and len(s) > 20:  # pure ID
        return s, "0"
    u = urlparse(s)
    parts = [p for p in u.path.split("/") if p]
    sid = parts[3] if len(parts) > 3 and parts[2] == "d" else parts[-1]
    gid = parse_qs(u.query).get("gid", ["0"])[0]
    return sid, gid

def facts_for(product: str) -> str:
    f = PRODUCT_FACTS.get(product or "", {})
    if not f: return ""
    return " | ".join([
        f"‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®: {f['indicative_return']}",
        f"‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ü ‡¶≤‡ßã‡¶°: {f['exit_load']}",
        f"{f['sip']}", f"{f['non_sip']}", f"{f['tax']}"
    ])

# =========================
# External loaders (Doc/Sheet)
# =========================
def download_docx_from_gdoc_id(doc_id: str, out_path: str) -> str:
    url = f"https://docs.google.com/document/d/{doc_id}/export?format=docx"
    return gdown.download(url, out_path, quiet=True)

def download_docx_from_link(link: str, out_path: str) -> str:
    # Convert Google Doc view link to export if needed
    if "docs.google.com/document/d/" in link and "export?format=docx" not in link:
        try:
            doc_id = link.split("/document/d/")[1].split("/")[0]
            link = f"https://docs.google.com/document/d/{doc_id}/export?format=docx"
        except Exception:
            pass
    r = requests.get(link, timeout=30)
    r.raise_for_status()
    with open(out_path, "wb") as f:
        f.write(r.content)
    return out_path

def extract_style_shots_from_docx(doc_path: str, max_shots: int = 4):
    if not os.path.exists(doc_path): return []
    doc = Document(doc_path)
    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
    cleaned = []
    for p in paragraphs:
        pp = _paragraphize(p)
        if len(pp) >= 180 and not re.match(r"^[‚Ä¢\-\d]+", pp):
            cleaned.append(pp)
    if not cleaned: return []
    if len(cleaned) <= max_shots: return cleaned
    step = max(1, len(cleaned)//max_shots)
    return [cleaned[i] for i in range(0, len(cleaned), step)][:max_shots]

def load_gsheet_df(url_or_id: str, gid_hint: str = "0") -> pd.DataFrame:
    sid, gid = _sheet_id_and_gid(url_or_id)
    gid = gid or gid_hint or "0"
    csv_url = f"https://docs.google.com/spreadsheets/d/{sid}/export?format=csv&gid={gid}"
    return pd.read_csv(csv_url)

def samples_from_df(df: pd.DataFrame):
    cols = {c.lower().strip(): c for c in df.columns}
    need = {"intent", "product", "script"}
    if not need.issubset(set(cols.keys())):
        raise ValueError(f"Sheet must contain columns: {sorted(need)}. Found: {list(df.columns)}")
    recs = df[[cols["intent"], cols["product"], cols["script"]]].fillna("")
    samples = {}
    for _, row in recs.iterrows():
        intent  = str(row[cols["intent"]]).strip()
        product = str(row[cols["product"]]).strip() or "‚Äî"
        script  = _paragraphize(str(row[cols["script"]]))
        if not intent or not script:
            continue
        samples.setdefault(intent, []).append({"product": product, "script": script})
    if not samples:
        raise ValueError("No valid rows (need non-empty intent + script).")
    return samples

# ---------------- Model / Generator (local-or-remote) ----------------
import os, sys, json, time
import requests
import streamlit as st

MODEL_NAME = "google/flan-t5-small"
HF_API_URL = f"https://api-inference.huggingface.co/models/{MODEL_NAME}"
HF_TOKEN = os.environ.get("HUGGINGFACEHUB_API_TOKEN", "")

def _have_torch() -> bool:
    try:
        import torch  # noqa
        return True
    except Exception:
        return False

@st.cache_resource(show_spinner=False)
def get_generator():
    """
    Returns a callable generate(prompt, **params) that abstracts away
    local vs remote inference.
    """
    if _have_torch():
        # --- Local pipeline (fastest if torch exists) ---
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
        tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        mdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
        pipe = pipeline("text2text-generation", model=mdl, tokenizer=tok, device_map="cpu")

        def _gen_local(prompt, max_new_tokens=500, temperature=0.8, top_p=0.95, top_k=50, repetition_penalty=1.05):
            out = pipe(
                prompt,
                max_new_tokens=int(max_new_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                top_k=int(top_k),
                repetition_penalty=float(repetition_penalty),
            )[0]["generated_text"]
            return out

        st.info("‚öôÔ∏è Using local Transformers pipeline (PyTorch detected).", icon="‚öôÔ∏è")
        return _gen_local

    # --- Remote inference (no torch needed) ---
    headers = {"Accept": "application/json"}
    if HF_TOKEN:
        headers["Authorization"] = f"Bearer {HF_TOKEN}"

    def _gen_remote(prompt, max_new_tokens=500, temperature=0.8, top_p=0.95, top_k=50, repetition_penalty=1.05):
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": int(max_new_tokens),
                "temperature": float(temperature),
                "top_p": float(top_p),
                "repetition_penalty": float(repetition_penalty),
            },
            "options": {"wait_for_model": True, "use_cache": True}
        }
        try:
            r = requests.post(HF_API_URL, headers=headers, json=payload, timeout=60)
            r.raise_for_status()
            data = r.json()
            # API returns a list of dicts with 'generated_text'
            if isinstance(data, list) and data and "generated_text" in data[0]:
                return data[0]["generated_text"]
            # Some endpoints use {'generated_text': '...'}
            if isinstance(data, dict) and "generated_text" in data:
                return data["generated_text"]
            # Or {'error': '...'}
            if isinstance(data, dict) and "error" in data:
                raise RuntimeError(data["error"])
            return json.dumps(data, ensure_ascii=False)
        except requests.exceptions.ReadTimeout:
            raise RuntimeError("HF inference timeout. Try again or reduce max tokens.")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"HF inference error: {e}")

    st.warning("üåê Using Hugging Face Inference API (no PyTorch). "
               "Add a HUGGINGFACEHUB_API_TOKEN in app secrets for better reliability.", icon="üåê")
    return _gen_remote





# =========================
# Prompting (two-pass + anti-copy + variability)
# =========================
def _facts_block(product: str, include: bool) -> str:
    if not include:
        return ""
    ftxt = facts_for(product)
    return f"\n[FACTS]\n{ftxt}\n[/FACTS]\n" if ftxt else ""

def pick_style_shots(style_shots: list, k: int = 3) -> list:
    """Randomly rotate which style exemplars are used to avoid sameness."""
    if not style_shots:
        return []
    k = max(1, min(k, len(style_shots)))
    return random.sample(style_shots, k)

def build_body_prompt(
    selected_shots,
    intent_sample,
    product,
    client_type,
    horizon,
    risk,
    extra,
    tone,
    include_facts=True,
    nonce=""
):
    styled = ""
    if selected_shots:
        blocks = [f"‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ {i} (‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞; ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ):\n{s}\n" for i, s in enumerate(selected_shots, 1)]
        styled = "\n".join(blocks)

    ex = _paragraphize(intent_sample or "")

    tone_rule = {
        "Factual": "‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§, ‡¶§‡¶•‡ßç‡¶Ø‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ì ‡¶®‡¶ø‡¶∞‡¶™‡ßá‡¶ï‡ßç‡¶∑ ‡¶•‡¶æ‡¶ï‡ßÅ‡¶®‡•§",
        "Elaborated": "‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï, ‡¶∏‡¶π‡¶æ‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø‡¶∂‡ßÄ‡¶≤ ‡¶ì ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï; ‡ßß‚Äì‡ß®‡¶ü‡¶ø ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£/‡¶∏‡¶ø‡¶®‡¶æ‡¶∞‡¶ø‡¶ì ‡¶¶‡¶ø‡¶®‡•§",
        "Sales Pitch": "‡¶Ü‡¶∏‡ßç‡¶•‡¶æ‡¶ú‡¶®‡¶ï ‡¶ì ‡¶™‡ßç‡¶∞‡¶∞‡ßã‡¶ö‡¶ø‡¶§; ‡¶ó‡ßç‡¶∞‡¶æ‡¶π‡¶ï‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶§‡¶¨‡ßÅ ‡¶¨‡¶æ‡¶°‡¶º‡¶æ‡¶¨‡¶æ‡¶°‡¶º‡¶ø ‡¶®‡¶Ø‡¶º‡•§"
    }.get(tone, "‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶ü‡ßã‡¶®‡•§")

    rules = [
        "‡¶≠‡¶æ‡¶∑‡¶æ: ‡¶ñ‡¶æ‡¶Å‡¶ü‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ; ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡•§",
        "‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø: ‡ß™‡ß´‡ß¶‚Äì‡ß≠‡ß´‡ß¶ ‡¶∂‡¶¨‡ßç‡¶¶; ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡•§",
        "‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã: ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ+‡¶°‡¶ø‡¶∏‡¶ï‡¶≠‡¶æ‡¶∞‡¶ø ‚Üí ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‚Üí ‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø-‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‚Üí ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£/‡¶∏‡¶ø‡¶®‡¶æ‡¶∞‡¶ø‡¶ì ‚Üí ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ß‡¶æ‡¶™ ‚Üí CTA‡•§",
        "‡¶®‡¶ø‡¶ö‡ßá‡¶∞ [FACTS] ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ï‡ßá‡¶¨‡¶≤ ‡¶∞‡ßá‡¶´‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏; ‡¶ï‡ßã‡¶®‡ßã ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º [FACTS]/[/FACTS] ‡¶¨‡¶æ '‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø' ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶ö‡ßç‡¶õ ‡¶¨‡¶°‡¶ø‡¶§‡ßá ‡¶≤‡¶ø‡¶ñ‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§",
        "‚Äò‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø‚Äô ‡¶¨‡¶æ ‚Äò‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø ‡¶®‡ßá‡¶á‚Äô ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶¶‡¶æ‡¶¨‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§",
        "‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø/‡¶Ö‡¶®‡ßÅ‡¶ö‡ßç‡¶õ‡ßá‡¶¶ ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ‚Äî‡¶®‡¶ø‡¶ú‡¶∏‡ßç‡¶¨ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá ‡¶®‡¶§‡ßÅ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶≤‡¶ø‡¶ñ‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§",
        tone_rule,
        f"‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶ô‡ßç‡¶ó‡¶ø‡¶§‡ßá ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶® ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶Ü‡¶á‡¶°‡¶ø: {nonce})."
    ]

    # ‚úÖ Prebuild strings so there are no backslashes inside { ... } expressions.
    nl = "\n"
    extra_hint = f"‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ ‡¶π‡¶ø‡¶®‡ßç‡¶ü (‡¶ï‡¶™‡¶ø ‡¶®‡¶Ø‡¶º):{nl}{ex}" if ex else ""
    facts_txt = _facts_block(product, include_facts)
    rules_joined = nl.join(rules)

    # Build final prompt (no backslashes inside {...})
    prompt = (
        "‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û ‡¶Æ‡¶ø‡¶â‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤ ‡¶´‡¶æ‡¶®‡ßç‡¶° RM‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‚Äî"
        "‡¶®‡¶ø‡¶ú‡¶∏‡ßç‡¶¨ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá, ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶ó‡¶†‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶æ‡¶ô‡ßç‡¶ó ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®‡•§"
        f"{nl}{nl}"
        f"{styled}{nl if styled else ''}"
        f"{extra_hint}{nl if extra_hint else ''}{nl}"
        f"{facts_txt}{nl if facts_txt else ''}"
        "‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:"
        f"{nl}- {rules_joined}"
        f"{nl}{nl}"
        "‡¶á‡¶®‡¶™‡ßÅ‡¶ü:"
        f"{nl}- ‡¶ï‡ßç‡¶≤‡¶æ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶ü‡¶æ‡¶á‡¶™: {client_type}"
        f"{nl}- ‡¶™‡¶£‡ßç‡¶Ø: {product}"
        f"{nl}- ‡¶∏‡¶Æ‡¶Ø‡¶º‡¶∏‡ßÄ‡¶Æ‡¶æ: {horizon}"
        f"{nl}- ‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø: {risk}"
        f"{nl}- ‡¶®‡ßã‡¶ü: {extra}"
        f"{nl}{nl}"
        "‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü:\n‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶¨‡¶°‡¶ø ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®; '‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø' ‡¶Ö‡¶Ç‡¶∂‡¶ü‡¶ø ‡¶è‡¶ñ‡¶® ‡¶≤‡¶ø‡¶ñ‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§"
    ).strip()

    return prompt


def _fallback_body(prod, horizon):
    greeting = "‡¶Ü‡¶∏‡¶∏‡¶æ‡¶≤‡¶æ‡¶Æ‡ßÅ ‡¶Ü‡¶≤‡¶æ‡¶á‡¶ï‡ßÅ‡¶Æ‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶á‡¶â‡¶∏‡¶ø‡¶¨‡¶ø ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡ßá‡¶ü ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶ú‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶≤‡¶õ‡¶ø‡•§"
    discovery = "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø, ‡¶∏‡¶Æ‡¶Ø‡¶º‡¶∏‡ßÄ‡¶Æ‡¶æ ‡¶ì ‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶¨‡ßÅ‡¶ù‡ßá ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶™‡¶∞‡¶ø‡¶ï‡¶≤‡ßç‡¶™‡¶®‡¶æ ‡¶∏‡¶æ‡¶ú‡¶æ‡¶¨‡ßã‡•§"
    explain = f"{prod} ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™‡ßá‚Äî‡¶™‡ßá‡¶∂‡¶æ‡¶¶‡¶æ‡¶∞ ‡¶ü‡¶ø‡¶Æ ‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø‚Äì‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∞‡¶∏‡¶æ‡¶Æ‡ßç‡¶Ø ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡ßá‡¶ñ‡ßá ‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó ‡¶ï‡¶∞‡ßá; {horizon} ‡¶¶‡¶ø‡¶ó‡¶®‡ßç‡¶§‡ßá ‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶æ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∏‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶π‡¶Ø‡¶º‡•§"
    risk_note = "‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶ì‡¶†‡¶æ‡¶®‡¶æ‡¶Æ‡¶æ ‡¶∏‡ßç‡¶¨‡¶æ‡¶≠‡¶æ‡¶¨‡¶ø‡¶ï; ‡¶™‡¶∞‡¶ø‡¶ï‡¶≤‡ßç‡¶™‡¶ø‡¶§ SIP/‡¶≤‡¶æ‡¶Æ‡ßç‡¶™‡¶∏‡¶æ‡¶Æ ‡¶Æ‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡¶≤‡ßá ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø‡¶™‡ßÇ‡¶∞‡¶£ ‡¶∏‡¶π‡¶ú ‡¶π‡¶Ø‡¶º (‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø ‡¶®‡¶Ø‡¶º)‡•§"
    steps = "‡¶ß‡¶æ‡¶™: (‡ßß) KYC/‡¶´‡¶∞‡ßç‡¶Æ (‡ß®) ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶´‡¶æ‡¶∞ ‡¶¨‡¶æ SIP ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (‡ß©) ‡¶ï‡¶®‡¶´‡¶æ‡¶∞‡ßç‡¶Æ‡ßá‡¶∂‡¶® ‡¶ì ‡¶∏‡ßç‡¶ü‡ßá‡¶ü‡¶Æ‡ßá‡¶®‡ßç‡¶ü (‡ß™) ‡¶∞‡¶ø‡¶≠‡¶ø‡¶â‡•§"
    cta = "‡¶Ü‡¶ú ‡¶ï‡¶ø ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡¶Ö‡¶Ç‡¶ï‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶¨‡ßã? ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ñ‡¶®‡¶á ‡¶¨‡ßç‡¶∞‡ßã‡¶∂‡¶ø‡¶ì‡¶∞/‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶™‡¶æ‡¶†‡¶ø‡¶Ø‡¶º‡ßá ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶≤‡ßã-‡¶Ü‡¶™ ‡¶ï‡¶≤ ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶õ‡¶ø‡•§"
    return "\n\n".join([greeting, discovery, explain, risk_note, steps, cta])

def _too_similar_to_any(body: str, shots: list, thresh: float = 0.78) -> bool:
    if not body or not shots: return False
    return any(_similar(body, s) >= thresh for s in shots)

def _too_similar_to_recent(body: str, history: list, thresh: float = 0.76) -> bool:
    for prev in history[-5:]:
        if _similar(body, prev) >= thresh:
            return True
    return False

def generate_script(style_shots, intent_sample, ct, prod, horizon, risk, extra, temp, max_tok, include_facts, tone):
    def _run(prompt, temperature):
        params = dict(
            max_new_tokens=int(max_tok),
            temperature=float(temperature),
            top_p=0.92,
            top_k=60,
            do_sample=True,
            repetition_penalty=1.12,
            no_repeat_ngram_size=4
        )
        return gen(prompt, **params)[0]["generated_text"].strip()

    # rotate style exemplars + add nonce per run
    selected = pick_style_shots(style_shots, k=min(3, len(style_shots) or 1))
    nonce = f"{int(time.time()*1000) % 100000}-{random.randint(100,999)}"

    body_prompt = build_body_prompt(selected, intent_sample, prod, ct, horizon, risk, extra, tone, include_facts, nonce=nonce)

    tries = 0
    body = ""
    while tries < 2:
        tries += 1
        try:
            body = _run(body_prompt, temp if tries == 1 else max(1.05, float(temp) + 0.25))
        except Exception:
            body = ""

        bad = (
            not _len_ok(body) or
            "[FACTS]" in body or "[/FACTS]" in body or
            "‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø" in body or
            _too_similar_to_any(body, (style_shots or []) + ([intent_sample] if intent_sample else []), 0.78) or
            _too_similar_to_recent(body, st.session_state.GEN_HISTORY, 0.76)
        )
        if not bad:
            break

        body_prompt += "\n\n‡¶™‡ßÅ‡¶®‡¶∞‡ßç‡¶≤‡¶ø‡¶ñ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂: ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶∞‡¶£ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã, ‡¶Ö‡¶®‡ßÅ‡¶ö‡ßç‡¶õ‡ßá‡¶¶ ‡¶ì ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®‚Äî‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∂‡¶¨‡ßç‡¶¶‡¶ö‡¶Ø‡¶º‡¶® ‡¶ì ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶ó‡¶†‡¶® ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®‡•§"

    if (not _len_ok(body) or
        "[FACTS]" in body or "[/FACTS]" in body or
        "‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø" in body or
        _too_similar_to_any(body, (style_shots or []) + ([intent_sample] if intent_sample else []), 0.78) or
        _too_similar_to_recent(body, st.session_state.GEN_HISTORY, 0.76)):
        body = _fallback_body(prod, horizon)

    body = re.sub(r"\[/?FACTS\]", "", body, flags=re.I)

    # Append facts & disclaimer (verbatim)
    tail = ""
    if include_facts:
        tail += "\n\n‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø (‡¶π‡ßÅ‡¶¨‡¶π‡ßÅ): " + facts_for(prod)
    tail += "\n\n‡¶®‡ßã‡¶ü: ‡¶Æ‡¶ø‡¶â‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤ ‡¶´‡¶æ‡¶®‡ßç‡¶° ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞; ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡ßá‡¶∞ ‡¶Ü‡¶Ø‡¶º ‡¶≠‡¶¨‡¶ø‡¶∑‡ßç‡¶Ø‡¶§‡ßá‡¶∞ ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶Ø‡¶º‡¶§‡¶æ ‡¶®‡¶Ø‡¶º‡•§"

    final = (body.strip() + tail)

    # remember this body so the next run avoids repeating it
    st.session_state.GEN_HISTORY.append(body.strip())
    if len(st.session_state.GEN_HISTORY) > 6:
        st.session_state.GEN_HISTORY = st.session_state.GEN_HISTORY[-6:]

    return final

# =========================
# Session state init
# =========================
if "STYLE_SHOTS" not in st.session_state: st.session_state.STYLE_SHOTS = []
if "SAMPLES" not in st.session_state:     st.session_state.SAMPLES = {}
if "GEN_HISTORY" not in st.session_state: st.session_state.GEN_HISTORY = []
# --- previews for UX ---
if "STYLE_PREVIEW" not in st.session_state: st.session_state.STYLE_PREVIEW = ""
if "SHEET_PREVIEW" not in st.session_state: st.session_state.SHEET_PREVIEW = {"intent": "", "product": "", "script": ""}

def ensure_style_loaded(doc_id: str, doc_link: str):
    out_path = "master_style.docx"
    if doc_id:
        download_docx_from_gdoc_id(doc_id, out_path)
    elif doc_link:
        download_docx_from_link(doc_link, out_path)
    else:
        return False
    if os.path.exists(out_path):
        st.session_state.STYLE_SHOTS = extract_style_shots_from_docx(out_path, max_shots=4)
        return bool(st.session_state.STYLE_SHOTS)
    return False

def ensure_samples_loaded(sheet_id: str, sheet_link: str, gid: str = "0"):
    if sheet_id:
        df = load_gsheet_df(sheet_id, gid_hint=gid or "0")
    elif sheet_link:
        df = load_gsheet_df(sheet_link, gid_hint="0")
    else:
        return False
    st.session_state.SAMPLES = samples_from_df(df)
    return True

# =========================
# UI
# =========================
st.set_page_config(page_title="AI Script Generator (Bangla)", layout="wide")
st.title("ü§ñ AI Script Generator (Bangla)")
st.caption("External-only: style from Google Doc, samples from Google Sheet")

with st.expander("üîó Connect your sources", expanded=True):
    c1, c2 = st.columns(2)
    with c1:
        doc_in = st.text_input("Google Doc ID or direct DOCX link", value=MASTER_DOC_ID or MASTER_DOC_LINK)
        if st.button("Load Style (Docx)"):
            with st.spinner("Fetching DOCX..."):
                ok = ensure_style_loaded(
                    doc_id=doc_in if "/" not in (doc_in or "") else "",
                    doc_link=doc_in if "/" in (doc_in or "") else ""
                )
            if ok:
                n = len(st.session_state.STYLE_SHOTS)
                st.session_state.STYLE_PREVIEW = (st.session_state.STYLE_SHOTS[0] or "")[:400]
                st.success(f"‚úÖ Loaded {n} style paragraph{'s' if n>1 else ''}.")
            else:
                st.session_state.STYLE_PREVIEW = ""
                st.error("‚ùå Failed to load style ‚Äî ensure the Doc is shared: ‚ÄòAnyone with the link ‚Üí Viewer‚Äô.")
            st.rerun()
    with c2:
        sheet_in = st.text_input("Google Sheet ID or link (headers: intent, product, script)", value=GSHEET_ID or GSHEET_LINK)
        gid_in   = st.text_input("Sheet gid (tab id, default 0)", value=GSHEET_GID or "0")
        if st.button("Load Samples (Sheet)"):
            with st.spinner("Fetching Sheet..."):
                try:
                    ok = ensure_samples_loaded(
                        sheet_id=sheet_in if "/" not in (sheet_in or "") else "",
                        sheet_link=sheet_in if "/" in (sheet_in or "") else "",
                        gid=gid_in or "0"
                    )
                except Exception as e:
                    ok = False
                    st.error(f"‚ùå Error while loading sheet: {e}")
            if ok:
                total_rows = sum(len(v) for v in st.session_state.SAMPLES.values())
                intents = len(st.session_state.SAMPLES)
                first_intent = next(iter(st.session_state.SAMPLES.keys()))
                first_row = st.session_state.SAMPLES[first_intent][0]
                st.session_state.SHEET_PREVIEW = {
                    "intent": first_intent,
                    "product": first_row.get("product", ""),
                    "script": (first_row.get("script", "") or "")[:400],
                }
                st.success(f"‚úÖ Loaded {intents} intents / {total_rows} rows from sheet.")
            else:
                st.session_state.SHEET_PREVIEW = {"intent":"", "product":"", "script":""}
                st.warning("‚ö†Ô∏è Could not load sheet ‚Äî confirm link/ID and public sharing.")
            st.rerun()

# Status + previews
st.info(f"Style shots: {len(st.session_state.STYLE_SHOTS)} | Intents: {len(st.session_state.SAMPLES)}")

with st.expander("üëÄ Loaded content preview", expanded=True):
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**Master style (from Google Doc)**")
        if st.session_state.STYLE_PREVIEW:
            st.code(st.session_state.STYLE_PREVIEW, language="markdown")
        else:
            st.info("No style loaded yet.")
    with c2:
        st.markdown("**Sample row (from Google Sheet)**")
        sp = st.session_state.SHEET_PREVIEW
        if sp.get("intent"):
            st.write(f"**Intent:** {sp['intent']}")
            st.write(f"**Product:** {sp['product']}")
            st.code(sp["script"], language="markdown")
        else:
            st.info("No sheet loaded yet.")

# Controls (enabled only when both sources exist)
can_generate = bool(st.session_state.STYLE_SHOTS and st.session_state.SAMPLES)

with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    if can_generate:
        ct = st.selectbox("‡¶ï‡ßç‡¶≤‡¶æ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶ü‡¶æ‡¶á‡¶™", list(st.session_state.SAMPLES.keys()))
        products = [r.get("product","‚Äî") for r in st.session_state.SAMPLES.get(ct, [])]
        prod = st.selectbox("‡¶™‡¶£‡ßç‡¶Ø/‡¶´‡ßã‡¶ï‡¶æ‡¶∏", products or ["‚Äî"])
        intent_sample = next((r.get("script","") for r in st.session_state.SAMPLES.get(ct, []) if r.get("product")==prod), "") \
                        or st.session_state.SAMPLES.get(ct, [{}])[0].get("script","")
    else:
        ct = prod = intent_sample = ""

    horizon = st.selectbox("‡¶∏‡¶Æ‡¶Ø‡¶º‡¶∏‡ßÄ‡¶Æ‡¶æ", ["‡ß¨‚Äì‡ßß‡ß® ‡¶Æ‡¶æ‡¶∏","‡ßß‚Äì‡ß© ‡¶¨‡¶õ‡¶∞","‡ß©+ ‡¶¨‡¶õ‡¶∞"])
    risk = st.radio("‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø", ["‡¶ï‡¶Æ","‡¶Æ‡¶ß‡ßç‡¶Ø‡¶Æ","‡¶â‡¶ö‡ßç‡¶ö"], horizontal=True)
    extra = st.text_area("‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶®‡ßã‡¶ü", "SIP ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞, ‡¶∂‡¶∞‡ßÄ‡¶Ø‡¶º‡¶æ‡¶π ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø")
    tone = st.selectbox("Script Tone", ["Elaborated","Factual","Sales Pitch"])
    temp = st.slider("Temperature", 0.3, 1.5, 0.95, 0.05)
    max_tok = st.slider("Max tokens", 450, 900, 700, 50)
    include_facts = st.checkbox("‡¶™‡¶£‡ßç‡¶Ø-‡¶§‡¶•‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶π‡ßÅ‡¶¨‡¶π‡ßÅ)", value=True)
    force_variant = st.checkbox("üîÅ Force a fresh variant", value=False)

st.markdown("### ‚ú® Generated Script")
btn = st.button("Generate Script", disabled=not can_generate)
if not can_generate:
    st.warning("Connect both: a Google Doc (style) and a Google Sheet (samples) to enable generation.")

if btn and can_generate:
    with st.spinner("AI generating your script..."):
        out = generate_script(
            style_shots=st.session_state.STYLE_SHOTS,
            intent_sample=intent_sample,
            ct=ct, prod=prod, horizon=horizon, risk=risk, extra=extra,
            temp=(temp + 0.25 if force_variant else temp),
            max_tok=max_tok, include_facts=include_facts, tone=tone
        )
        st.text_area("Generated Script", out, height=600)
        st.download_button("‚¨áÔ∏è Download .txt", out.encode("utf-8"), "script.txt")

st.markdown("---")
st.caption("¬© UCB Asset Management Ltd | External-data-driven ‚Äî no in-code samples")





